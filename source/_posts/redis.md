---
title: redis
date: 2020-12-04 10:34:34
categories: Nosql
tags: 缓存
---

#  redis基础知识

- **Key-Value数据库，NOSQL**
- **存储，查询高效(Redis读的速度是110000次/秒,写的速度80000次/秒)**
- **单线程，基于内存操作，CPU不是redis性能瓶颈，瓶颈是机器带宽和内存**

##  存储数据结构

**String类型**

**这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存**

String的实际应用比较广泛

- **缓存功能:****String**字符串是最常用的数据类型，不仅仅是**Redis**，各个语言都是最基本类型，因此，利用**Redis**作为缓存，配合其它数据库作为存储层，利用**Redis**支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。
- **计数器:**许多系统都会使用**Redis**作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。
- **共享用户Session：**用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存**Cookie**，但是可以利用**Redis**将用户的**Session**集中管理，在这种模式只需要保证**Redis**的高可用，每次用户**Session**的更新和获取都可以快速完成。大大提高效率。

**List列表**

 比如可以通过 **List** 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。

- **消息队列：Redis**的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过**Lpush**命令从左边插入数据，多个数据消费者，可以使用**BRpop**命令阻塞的“抢”列表尾部的数据。

- 文章列表或者数据分页展示的应用。

  比如，我们常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用**Redis**的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。

**Set集合**

**Set** 是无序集合，会自动去重的那种

可以基于 **Set** 玩儿交集、并集、差集的操作，比如交集吧，我们可以把两个人的好友列表整一个交集，看看俩人的共同好友是谁

**Hash类型**

> key-value

**Zset有序集合**

**Sorted set** 是排序的 **Set**，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。

有序集合的使用场景与集合类似，但是set集合不是自动有序的，而**Sorted set**可以利用分数进行成员间的排序，而且是插入时就排序好。所以当你需要一个有序且不重复的集合列表时，就可以选择**Sorted set**数据结构作为选择方案。

- **排行榜**：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。

- 用**Sorted Sets**来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。

  微博热搜榜，就是有个后面的热度值，前面就是名称

**Geospatial地理位置**

代补充

**Hyperloglog基数统计**

**通常是用来统计一个集合中不重复的元素个数**

> 如果统计 PV 那非常好办，给每个网页一个独立的 Redis 计数器就可以了，这个计数器的 key 后缀加上当天的日期。这样来一个请求，incrby 一次，最终就可以统计出所有的 PV 数据。
>
> 但是 UV 不一样，它要去重，同一个用户一天之内的多次访问请求只能计数一次。
>
> 这就要求每一个网页请求都需要带上用户的 ID，无论是登陆用户还是未登陆用户都需要一个唯一 ID 来标识
>
> 你也许已经想到了一个简单的方案，那就是为每一个页面一个独立的 set 集合来存储所有当天访问过此页面的用户 ID。
>
> 当一个请求过来时，我们使用 sadd 将用户 ID 塞进去就可以了。
>
> 通过 scard 可以取出这个集合的大小，这个数字就是这个页面的 UV 数据。没错，这是一个非常简单的方案。
>
> 但是，如果你的页面访问量非常大，比如一个爆款页面几千万的 UV，你需要一个很大的 set 集合来统计，这就非常浪费空间。
>
> 如果这样的页面很多，那所需要的存储空间是惊人的。为这样一个去重功能就耗费这样多的存储空间，值得么？其实老板需要的数据又不需要太精确，105w 和 106w 这两个数字对于老板们来说并没有多大区别，So，有没有更好的解决方案呢？
>
> HyperLogLog 提供了两个指令 pfadd 和 pfcount，根据字面意义很好理解，一个是增加计数，一个是获取计数。
>
> pfadd 用法和 set 集合的 sadd 是一样的，来一个用户 ID，就将用户 ID 塞进去就是，pfcount 和 scard 用法是一样的，直接获取计数值。

~~~java
127.0.0.1:6379> pfadd codehole user1
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 1
127.0.0.1:6379> pfadd codehole user2
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 2
127.0.0.1:6379> pfadd codehole user3
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 3
127.0.0.1:6379> pfadd codehole user4
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 4
127.0.0.1:6379> pfadd codehole user5
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 5
127.0.0.1:6379> pfadd codehole user6
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 6
127.0.0.1:6379> pfadd codehole user7 user8 user9 user10
(integer) 1
127.0.0.1:6379> pfcount codehole
(integer) 10
~~~

**Hyperloglog原理----->** https://mp.weixin.qq.com/s/9dtGe3d_mbbxW5FpVPDNow

**BitMap位图场景**

> 位存储,都是操作二进制进行记录,0和1
>
> Redis提供了SETBIT、GETBIT、BITCOUNT、BITOP四个命令用于处理二进制位数组。

## BitMap原理

> BitMap的基本原理就是用一个bit来标记某个元素对应的value,而key即是该元素。由于采用一个bit来存储一个数据，因此可以大大的节省空间。
>
> 我们通过一个具体例子来说明BitMap的原理。假设我们要对0-31内的3个元素(10,17,28)排序。我们就可以采用BitMap方法(假设这些元素没有重复)
>
> 如下图,要表示32个数，我们只需要32个bit(4Bytes)，首先我们开辟4Byte的空间,将这些空间的所有bit位都设置为0

![image-20201204150535800](https://jameslin23.gitee.io/2020/12/04/redis/image-20201204150535800.png)

然后，我们要添加(10, 17,28) 这三个数到 BitMap 中，需要的操作就是在相应的位置上将0置为1即可。如下图，比如现在要插入 10 这个元素，只需要将蓝色的那一位变为1即可。

![image-20201204150638693](https://jameslin23.gitee.io/2020/12/04/redis/image-20201204150638693.png)

将这些数据插入后，假设我们想对数据进行排序或者检索数据是否存在，就可以依次遍历这个数据结构，碰到位为 1 的情况，就当这个数据存在。

**字符串映射**

> BitMap 也可以用来表述字符串类型的数据，但是需要有一层Hash映射，如下图，通过一层映射关系，可以表述字符串是否存在。

![image-20201204150738688](https://jameslin23.gitee.io/2020/12/04/redis/image-20201204150738688.png)



当然这种方式会有数据碰撞的问题，但可以通过 Bloom Filter 做一些优化。

**使用场景一：统计活跃用户**

> 使用时间作为cacheKey，然后用户ID为offset(当用户很大时，需要优化)，如果当日活跃过就设置为1

**使用场景二：户在线状态**

> 只需要一个key，然后用户ID为offset(当用户很大时，需要优化)，如果在线就设置为1，不在线就设置为0

**使用场景三 : 用户签到**

> 用户ID作为key，签到的天数offest

#  redis持久化

**SAVE  保存是阻塞主进程，客户端无法连接redis，等SAVE完成后，主进程才开始工作，客户端可以连接**

**BGSAVE  是fork一个save的子进程，在执行save过程中，不影响主进程，客户端可以正常链接redis，等子进程fork执行save完成后，通知主进程，子进程关闭。很明显BGSAVE**

##  RDB

> 快照方式，允许你每隔一段时间对内存数据做一次快照然后存储到硬盘中

**RDB可以通过在配置文件中配置时间或者改动键的个数来定义快照条件，编辑配置文件redis.conf**

~~~conf
# 默认配置
save 900 1    #15分钟之内至少有一个键被更改则进行快照
save 300 10   #5分钟之内至少有10个键被更改则进行快照 
save 60 10000 #1分钟之内至少有1000个键被更改则进行快照
~~~

RDB持久化到磁盘文件默认路径是当前安装目录，文件名为dump.rdb，你可以通过配置文件dir和dbfilename来指定文件目录和文件名称，rdb文件还可以进行压缩,你可以通过配置rdbcompression参数进行压缩

![image-20201204162833185](https://jameslin23.gitee.io/2020/12/04/redis/image-20201204162833185.png)

**执行流程**

> （1）redis根据配置自己尝试去生成rdb快照文件
>
> （2）fork一个子进程出来
>
> （3）子进程尝试将数据dump到临时的rdb快照文件中
>
> （4）完成rdb快照文件的生成之后，就替换之前的旧的快照文件

**存在问题**

> 当还没进行sava，触发存储快照时，服务器宕机，会丢失上次保存快照-到宕机时这段时间的数据。

##  AOF

> 记录客户端对服务器的每一个写操作命令,并将这些写操作以Redis协议追加保存到.aof文件,在redis服务器重启时,在redis服务器重启时，会加载并运行aof文件命令，已达到恢复数据的目的。

![image-20201204162946548](https://jameslin23.gitee.io/2020/12/04/redis/image-20201204162946548.png)

**开启AOF持久化方式**

Redis默认不开启AOF持久化方式，我们可以在配置文件中开启并进行更加详细的配置，如下面的redis.conf文件：

```conf
# 开启aof机制
appendonly yes

# aof文件名
appendfilename "appendonly.aof"

# 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec或no
appendfsync always

# 默认不重写aof文件
no-appendfsync-on-rewrite no

# 保存目录
dir ~/redis/
```

**执行方式**

- **always**: Redis的每条写命令都写入到系统缓冲区，然后每条写命令都使用fsync“写入”硬盘。

  > 客户端的每一个写操作都保存到aof文件当，这种策略很安全，但是每个写请注都有IO操作，所以也很慢。

- **everysec**: 过程与always相同，只是fsync的频率为1秒钟一次。这个是Redis默认配置，如果系统宕机，会丢失一秒左右的数据

  > appendfsync的默认写入策略，每秒写入一次aof文件，因此，最多可能会丢失1s的数据。

- **no**: 由操作系统决定什么时候从系统缓冲区刷新到硬盘。

  > Redis服务器不负责写入aof，而是交由操作系统来处理什么时候写入aof文件。更快，但也是最不安全的选择，不推荐使用。

**AOF文件重写**

> AOF将客户端的每一个写操作都追加到aof文件末尾，比如对一个key多次执行incr命令，这时候，aof保存每一次命令到aof文件中，aof文件会变得非常大。

~~~java
incr num 1
incr num 2
incr num 3
...
incr num 100000
~~~

aof文件太大，加载aof文件恢复数据时，就会非常慢，为了解决这个问题，Redis支持aof文件重写，通过重写aof，可以生成一个恢复当前数据的最少命令集，比如上面的例子中那么多条命令，可以重写为:

```text
set num 100000
```

**注意：通过在redis.conf配置文件中的选项no-appendfsync-on-rewrite可以设置是否开启重写，这种方式会在每次fsync时都重写，影响服务器性以，因此默认值为no，不推荐使用。**

**AOF文件损坏**

> 在写入aof日志文件时，如果redis服务器宕机,则aof日志文件会出格式错误,在重启redis服务器时,redis服务器会拒绝载入这个aof文件，可以通过以下步骤修复aof并恢复数据
>
> 使用redis-check-aof命令修复aof文件，该命令格式如下
>
> ```text
> redis-check-aof -fix file.aof
> ```

**AOF的优点**

AOF只是追加日志文件，因此对服务器性能影响较小，速度比RDB要快，消耗的内存较小

**AOF的缺点**

- AOF方式生成的日志文件太大，即使通过AFO重写，文件体积仍然很大。
- 恢复数据的速度比RDB

##  RDB VS AOF

|            | RDB    | AOF      |
| ---------- | ------ | -------- |
| 启动优先级 | 低     | 高       |
| 体积       | 小     | 大       |
| 恢复速度   | 快     | 慢       |
| 数据安全性 | 丢数据 | 根据策略 |



#  小总结

- **简单概述redis基础，8大数据结构，及bitmap原理，运用场景**
- **redis两大持久化机制,RDB原理，AOF原理及各种优缺点**